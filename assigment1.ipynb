{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amambayeva/Amambayeva/blob/main/assigment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/InterimAssignmentHeader.png?raw=true\">"
      ],
      "metadata": {
        "id": "qE4LDUQhcK1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Overview</b>\n",
        "\n",
        "This interim assignment will focus on the topics that we covered in the first two units of this course.\n",
        "\n",
        "You will try to improve the performance of a bi-directional LSTM model that will learn to classify text into 2 sentiment categories: ```positive, negative```.\n",
        "\n",
        "Most of the code is prepared for you, so your job is to really focus on the model training and performance!\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>The Task</b>\n",
        "\n",
        "To improve model performance, you will adjust the <u>hyperparameters</u> and optionally the <u>optimizer</u> and <u>loss function</u>.\n",
        "\n",
        "Your aim is to end up with a model that fulfills these criteria:\n",
        "- Its accuracy on the validation data is within a generally accepted range (80% or higher)\n",
        "- It also perfroms well on test data (to show that the model didn't overfit on the training data)\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Before you get started</b>\n",
        "\n",
        "It might be worth changing the runtime settings (\"Change runtime type\") in the top-right hand corner of Google Colab to \"T4 GPU\". Depending on what hyperparameters you set, your Google Colab session might crash occasionally (if the settings require too much memory, e.g. a too large batch size). Using a GPU can help avoid session crashes (but even the GPU will reach its memory limits if you push the training too much with the hyperparameters).\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>What to Document, Save, and Submit</b>\n",
        "\n",
        "Please document your process (i.e. what your tried out and how it went), your reasoning, and your final results! When you are satisfied with your results, save and download your notebook and attach it to your submission!\n",
        "\n",
        "In your documentation, please make sure to also cover these questions:\n",
        "1. What is an obvious short-coming of this training data?\n",
        "2. Which hyperparameters seem to have the largest influence (i.e. caused the largest changes in the model training)?\n",
        "3. Give a brief description of overfitting and how you can tell if that happened to your model during training. If you observe overfitting in during experiment, include a screenshot of the accuracies chart to show it!\n",
        "4. Which hyperparameter settings did you ultimately choose for your submission (and how did you come to those final settings)?\n",
        "5. After you have satisfactorily trained the model, try to come up with some custom input text (at the bottom of this notebook) where the model obviously struggles - include a few examples in your submission and try to explain why the model might struggle.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Grading</b>\n",
        "\n",
        "For this interim assignment, you can get a total of 25 points. Here's the breakdown:\n",
        "\n",
        "<br>\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Task</th>\n",
        "    <th>Points</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Validation Acc. above 80%</td>\n",
        "    <td>6</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Test Acc. above 70%</td>\n",
        "    <td>7</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Reasoning / decisions</td>\n",
        "    <td>6</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Question answers and documentation</td>\n",
        "    <td>6</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "I2dYsBUBb8I6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- PREPARED FOR YOU, DO NOT EDIT --------------------------\n",
        "\n",
        "# General Data Handling and Import\n",
        "import xml.etree.cElementTree as ET                                             # Parses and creates XML documents. ET is a lightweight and efficient XML API\n",
        "import urllib.request                                                           # Module for opening URLs, mainly useful for reading data across the web\n",
        "import pandas as pd                                                             # Data manipulation and analysis library, offers data structures like DataFrame\n",
        "import numpy as np                                                              # Fundamental package for scientific computing with Python, supports large, multi-dimensional arrays and matrices\n",
        "\n",
        "\n",
        "# PyTorch Libraries\n",
        "import torch                                                                    # Main PyTorch library, used for building deep learning models\n",
        "import torch.nn as nn                                                           # Provides a set of modules and loss functions to build neural networks\n",
        "from torch import optim                                                         # Optimization algorithms like SGD, Adam, etc., for training models\n",
        "from torch.utils.data import Dataset, DataLoader                                # Utilities for wrapping data for training, such as batching, shuffling\n",
        "from torch.nn.utils.rnn import pad_sequence                                     # Utility function for padding sequences to the same length for batch processing\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Other Libraries useful for Natural Language Processing\n",
        "import nltk                                                                     # Natural Language Toolkit, a set of libraries for symbolic and statistical natural language processing\n",
        "nltk.download('punkt')                                                          # Downloads the Punkt tokenizer models, which is a pre-trained sentence tokenizer\n",
        "from nltk.tokenize import word_tokenize                                         # Tokenizes a text into a list of words, used for processing natural language text\n",
        "from gensim.models import Word2Vec                                              # Implementation of the Word2Vec word embedding model for generating word vectors\n",
        "import gensim.downloader as api                                                 # API for downloading pre-trained word embedding models from Gensim's repository\n",
        "from sklearn.model_selection import train_test_split                            # Utility function for splitting data arrays into training and testing subsets\n",
        "\n",
        "\n",
        "# Data Plotting\n",
        "import matplotlib.pyplot as plt                                                 # Library for creating static, animated, and interactive visualizations in Python\n",
        "\n",
        "\n",
        "# Counter for keeping track of things!\n",
        "from collections import Counter                                                 # Dict subclass for counting hashable objects, useful for creating vocabularies or counting item\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yicpp0A3bw47",
        "outputId": "64fa3ed4-b5f6-46cd-c277-e024e8557403"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Preparing the Training Data 📑\n",
        "\n",
        "*This secion is prepared for you - there's **no need to edit** this code, but you do need to **run the cells** and, ideally, take some time to **understand** what is going on!*\n"
      ],
      "metadata": {
        "id": "Lr3i1AXrcXeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we'll load the data from a URL. This dataset is called \"SemEval\", and includes text and a sentiment classification. We'll also turn the labels into numbers."
      ],
      "metadata": {
        "id": "n8mBGiEgcg37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- PREPARED FOR YOU, DO NOT EDIT --------------------------\n",
        "\n",
        "# URL for the data (this is a link to a CSV hosted on Google Drive)\n",
        "data_url = \"https://drive.google.com/file/d/1zy9GnyHNZlhuytN8tpTtNehONy1EX5qb/view?usp=sharing\"\n",
        "data_url ='https://drive.google.com/uc?id=' + data_url.split('/')[-2]\n",
        "\n",
        "# Read the data from the URL and turn it into a pandas DataFrame\n",
        "df = pd.read_csv(data_url, encoding='utf-8')\n",
        "print(f\"The whole dataset is quite large. {len(df)} rows to be exact!\\n\\nIf we use all of that data for model training, Google Colab will crash because it will run out of memory!\")\n",
        "print(\"Instead, we'll use a subset of just 5000 randomly sampled rows.\\n\")\n",
        "\n",
        "# Randomly sample 5000 rows from the df\n",
        "df = df.sample(5000)\n",
        "\n",
        "# Display the columns that this DataFrame has\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "sfYliHK7ccoq",
        "outputId": "bb235cac-0ca4-4eef-bd73-c44444fb514e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The whole dataset is quite large. 50000 rows to be exact!\n",
            "\n",
            "If we use all of that data for model training, Google Colab will crash because it will run out of memory!\n",
            "Instead, we'll use a subset of just 5000 randomly sampled rows.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Unnamed: 0                                               text  \\\n",
              "6849      1082829  really really happy with the LAKERS win!!! woo...   \n",
              "2427        99032  4 days till Philippines... Makati City we are ...   \n",
              "21180       68120                                  Oh thats so sad!    \n",
              "19613      499391  ummmmm how do u get the thing to send udates t...   \n",
              "40089     1578258  Good morning! How are my fellow McFly fans tod...   \n",
              "\n",
              "       sentiment  \n",
              "6849           1  \n",
              "2427           0  \n",
              "21180          0  \n",
              "19613          0  \n",
              "40089          1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61a0944f-e569-46f4-88e6-42c82e6319ac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6849</th>\n",
              "      <td>1082829</td>\n",
              "      <td>really really happy with the LAKERS win!!! woo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2427</th>\n",
              "      <td>99032</td>\n",
              "      <td>4 days till Philippines... Makati City we are ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21180</th>\n",
              "      <td>68120</td>\n",
              "      <td>Oh thats so sad!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19613</th>\n",
              "      <td>499391</td>\n",
              "      <td>ummmmm how do u get the thing to send udates t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40089</th>\n",
              "      <td>1578258</td>\n",
              "      <td>Good morning! How are my fellow McFly fans tod...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61a0944f-e569-46f4-88e6-42c82e6319ac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-61a0944f-e569-46f4-88e6-42c82e6319ac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-61a0944f-e569-46f4-88e6-42c82e6319ac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0295174b-4420-4494-8587-fdcf62facdca\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0295174b-4420-4494-8587-fdcf62facdca')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0295174b-4420-4494-8587-fdcf62facdca button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 465097,\n        \"min\": 284,\n        \"max\": 1599454,\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          760875,\n          1424715,\n          994982\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"@shanesss WIN! I'm proud of your converting skillz. There are like no comms or anything, it's so sad \",\n          \"@Megan_Lynn_O is like the coolest most awesomest person in the world. I love my bestest friend! \",\n          \"@1Superstar We're going to be good friends...Lol....Love ya! \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next up, we need to prepare the text data so that it can be turned into numbers (and ultimately tensors).\n",
        "\n",
        "Here is what we will do:\n",
        "1. **Tokenize the text**:this means we split the text into a list of individual tokens, which are usually just the individual words, but long words are sometimes also split into sub-words. To do this, we are using the function ```word_tokenize()```, which we can simply import from a Python package called ```nltk``` (by the way, nltk stands for natural language toolkit).\n",
        "2. **Build a vocab**: this is just a big dictionary where each word in the entire dataset is listed and given a unique number. Very simple!"
      ],
      "metadata": {
        "id": "3xBs9Crccbms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- PREPARED FOR YOU, DO NOT EDIT --------------------------\n",
        "\n",
        "# A function that will build the vocabulary\n",
        "def build_vocab(texts):\n",
        "    token_freqs = Counter()\n",
        "    for text in texts:\n",
        "        tokens = word_tokenize(text.lower())                                    # word_tokenize is an nltk library that breaks down text into tokens\n",
        "        token_freqs.update(tokens)                                              # keeping track of the frequency of each token\n",
        "    vocab = {token: idx + 1 for idx, token in enumerate(token_freqs)}           # Start indexing from 1\n",
        "    vocab['<pad>'] = 0                                                          # Padding token\n",
        "    return vocab\n",
        "\n",
        "# Here, we call the function and build the actual vocab\n",
        "vocab = build_vocab(df['text'])"
      ],
      "metadata": {
        "id": "-F2wQzVWctoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- PREPARED FOR YOU, DO NOT EDIT --------------------------\n",
        "\n",
        "# A function that will turn the SemEval text into a simple numeric format (each word is turned into a unqiue number using the vocab!)\n",
        "def tokenize_and_convert_to_indices(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [vocab.get(token, 0) for token in tokens]                            # Use 0 for unknown tokens\n",
        "\n",
        "# Create a new column in the Dataframe that contains the numeric format of the text\n",
        "df['indexed_text'] = df['text'].apply(tokenize_and_convert_to_indices)"
      ],
      "metadata": {
        "id": "3rmwc3hFc1fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- PREPARED FOR YOU, DO NOT EDIT --------------------------\n",
        "\n",
        "# Now, let's take a quick look at the new column\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "WB_LVsZwc1UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, when we get ready for training a model, we usually split the data into:\n",
        "* training data (70%)\n",
        "* validation data (10%)\n",
        "* testing data (20%)\n",
        "\n",
        "Below, we create those 3 subsets! Keep in mind that even after the split, they are still \"only\" pandas Dataframes, not yet PyTorch Datasets! That will come in the step afterwards."
      ],
      "metadata": {
        "id": "LFXhrc4NcwFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- PREPARED FOR YOU, DO NOT EDIT --------------------------\n",
        "\n",
        "# First split: Separate out the test dataset (20% of the total data)\n",
        "train_and_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Second split: Split the train_and_val_df into training and validation sets\n",
        "# Note: 0.125 of the remaining data (which is 80% of the total) will be 10% of the total dataset\n",
        "train_df, val_df = train_test_split(train_and_val_df, test_size=0.125, random_state=42)\n",
        "\n",
        "print(f\"The training dataset is now of size: {len(train_df)}\")\n",
        "print(f\"The validation dataset is now of size: {len(val_df)}\")\n",
        "print(f\"The testing dataset is now of size: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "Npkult8rc8pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next up: Creating a PyTorch **Dataset** object! <img src=\"https://cdn.icon-icons.com/icons2/2699/PNG/512/pytorch_logo_icon_169823.png\" align=\"right\" width=\"150px\">\n",
        "\n",
        "This should be somewhat familiar to you now! The Dataset stores the data in a way that is convenient for the entire pytorch library to work with!"
      ],
      "metadata": {
        "id": "_nLZlmpMc-7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- PREPARED FOR YOU, DO NOT EDIT --------------------------\n",
        "\n",
        "# Defining the pytorch Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = torch.tensor(self.df.iloc[idx]['indexed_text'], dtype=torch.long)\n",
        "        sentiment = torch.tensor(self.df.iloc[idx]['sentiment'], dtype=torch.long)\n",
        "        return text, sentiment\n",
        "\n",
        "\n",
        "# Turn our training, validation and test data into Dataset objects\n",
        "train_dataset = TextDataset(train_df)\n",
        "val_dataset = TextDataset(val_df)\n",
        "test_dataset = TextDataset(test_df)"
      ],
      "metadata": {
        "id": "MS45rGsidBFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next up: we define a funtion that will be needed for the DataLoader later. It prepares the batches by adding padding to all elements in the batch, so that they are all the same length. It will also put the sentiment label (which is either a 0 or a 1) in a tensor format."
      ],
      "metadata": {
        "id": "OPYaSmHDdD23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- PREPARED FOR YOU, DO NOT EDIT --------------------------\n",
        "\n",
        "# A function to prepare batches for input to the model\n",
        "# It will match the length of all inputs with padding and turns the labels into tensors\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Unzip the batch to separate the text and sentiment into individual variables\n",
        "    texts, sentiments = zip(*batch)\n",
        "\n",
        "    # Pad the sequence of texts so they all have the same length\n",
        "    texts = pad_sequence(texts, batch_first=True, padding_value=vocab['<pad>'])\n",
        "\n",
        "    # Convert the list of sentiment labels into a tensor of long integers\n",
        "    sentiments = torch.tensor(sentiments, dtype=torch.long)\n",
        "\n",
        "    # Return the padded texts and sentiment tensors\n",
        "    return texts, sentiments\n"
      ],
      "metadata": {
        "id": "W_87jxHsdGF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the embeddings, we will use a pre-trained embedding model called \"Word2Vec\". This model was trained to learn the semantics of words (sentence structure, word contexts, etc). This way, when the model turns text into embeddings, the numbers represent the semantic meaning! This is quite useful for any following tasks!"
      ],
      "metadata": {
        "id": "G0gJ-sIidIXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- PREPARED FOR YOU, DO NOT EDIT --------------------------\n",
        "\n",
        "# Download pre-trained Word2Vec embeddings trained on Google News corpus\n",
        "word2vec_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Assuming 'vocab' is a dictionary mapping your vocabulary words to unique indices\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300  # Dimensionality of Google News Word2Vec embeddings\n",
        "\n",
        "# Initialize an embedding matrix that will be used to set the weights in your embedding layer\n",
        "embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, index in vocab.items():\n",
        "    try:\n",
        "        # Update the row in embedding matrix with the Word2Vec vector if the word is found\n",
        "        embedding_matrix[index] = torch.tensor(word2vec_model[word])\n",
        "    except KeyError:\n",
        "        # If the word is not found in the Word2Vec model, initialize the row with random values\n",
        "        embedding_matrix[index] = torch.tensor(np.random.normal(scale=0.6, size=(embedding_dim, )))\n"
      ],
      "metadata": {
        "id": "VQNNXRDZdKvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. The Model 🤖\n",
        "\n",
        "*This secion is prepared for you - there's **no need to edit this code, <u>UNLESS</u> you want to adjust the model architecture and feel confident in doing so!** Either way, you do need to **run the cells** and, ideally, take some time to **understand** what is going on!*"
      ],
      "metadata": {
        "id": "qE5KSuV4dM9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, lstm_dim, hidden_dim1, hidden_dim2, lstm_layers, pretrained_embeddings, activation_function):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(pretrained_embeddings)             # Load the pre-trained embeddings from Word2Vec\n",
        "        self.embedding.weight.requires_grad = True                              # True means the embeddings will be fine-tuned as the model trains! (False would leave these as they are)\n",
        "\n",
        "        # Bi-Directional LSTM layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, lstm_dim, num_layers=lstm_layers, batch_first=True, dropout=0.5 if lstm_layers > 1 else 0, bidirectional=True)\n",
        "\n",
        "        # Linear layers\n",
        "        self.linear1 = nn.Linear(2 * lstm_dim, hidden_dim1)                     # *2 for bidirectional LSTM output\n",
        "        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.linear3 = nn.Linear(hidden_dim2, 2)\n",
        "\n",
        "        # Activation function\n",
        "        self.actfn = activation_function\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)                                                   # Embed the input batch\n",
        "        x, (hidden, cell) = self.lstm(x)                                        # Get the output from the LSTM\n",
        "        x = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)                # Use the last hidden state (from both directions, since this is a bi-directional LSTM)\n",
        "        x = self.dropout(self.actfn(self.linear1(x)))                           # Apply the linear layers and activation function\n",
        "        x = self.dropout(self.actfn(self.linear2(x)))\n",
        "        logits = self.linear3(x)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "R5-cV162dPDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Setting the Hyperparameters 🧰🛠\n",
        "\n",
        "❗ *This is the part where you need to make some decisions! Read the instructions below carefully.* ❗"
      ],
      "metadata": {
        "id": "lJDe5LevdSJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code-cell below, you see a list of hyperparameters. They are all set to a \"dummy\" value of ```1```. Your job is to change these values. You can also change the optimizer and the loss function!\n",
        "\n",
        "<br>\n",
        "\n",
        "After you have set the hyperparameters, you will run the training code-cell further down in this notebook. The code there is prepared and will automatically load the model with your hyperparametes and run the training. You will then be shown the statistics of your achieved loss and accuracy. The goal is to optimize these! After you've run the training code-cell and viewed the results, you can go back to the hyperparameter code-cell to adjust the hyperparameters and try the model training again. You can re-adjust the hyperparameters and then re-run the model training as often as you want!\n",
        "\n",
        "<br>\n",
        "\n",
        "A quick reminder: hyperparameters are the model and/or training settings that we can pre-define and they will influence either the model itself or how it is trained. Depending on the hyperparameters, the results of a model can vary a lot!\n",
        "\n",
        "<br>\n",
        "\n",
        "Your job is to experiment with the hyperparameter settings and to try to optimize the model so that it fulfills both of the criteria below:\n",
        "\n",
        "<br>\n",
        "\n",
        "1. **It achieves a training accuracy above 80%**\n",
        "2. **It does not overfit on the training data: The accuracy on the test data should be above 70%**\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "Here are a few links that might be useful for your hyperparameter setting:\n",
        "\n",
        "- Take a look at the available optimizers <a href=\"https://pytorch.org/docs/stable/optim.html\">here</a>\n",
        "\n",
        "- Take a look at the available loss functions <a href=\"https://pytorch.org/docs/stable/nn.functional.html#loss-functions\">here</a>, and for some additional information <a href=\"https://blog.paperspace.com/pytorch-loss-functions/\">here</a>\n",
        "\n",
        "- Take a look at the available activation functions <a href=\"https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\">here</a>"
      ],
      "metadata": {
        "id": "6WyhdmUkdS9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- ADJUST THESE PARAMETERS -----------------------\n",
        "\n",
        "# Hyperparameters for the model architecture\n",
        "lstm_layers =     1\n",
        "lstm_dim =        1\n",
        "activation_function = nn.ReLU()\n",
        "hidden_dim1 =     1\n",
        "hidden_dim2 =     1\n",
        "# output_dim is already set to 2 in the model architecture (since there are only two output classes!)\n",
        "\n",
        "# Hyperparameters for the training loop\n",
        "learning_rate =   1\n",
        "epochs =          1\n",
        "batch_size =      1\n",
        "\n",
        "def get_optimizer_and_loss(model):\n",
        "\n",
        "    # Change the optimizer if you want to (replace the \"AdamW\" with a different optimizer from torch.optim.)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Change the loss function if you want to (check out the loss functions on the torch.nn page)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    return optimizer, loss_fn\n"
      ],
      "metadata": {
        "id": "7_e-fiiLda66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. The Training Loop 📈🔁\n",
        "\n",
        "*This secion is prepared for you - there's **no need to edit** this code. This is where you will monitor how well the model performs on the training data and the validation data. The statistics at that will be created in the end will show you*\n"
      ],
      "metadata": {
        "id": "FPDALyjOdddP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- PREPARED FOR YOU, DO NOT EDIT --------------------------\n",
        "\n",
        "\n",
        "# ----------------- Prepare the Dataloader for Training -----------------------\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "# ------------------------ Instantiate the model ------------------------------\n",
        "\n",
        "assignment_model = Model(vocab_size, embedding_dim, lstm_dim, hidden_dim1, hidden_dim2, lstm_layers, embedding_matrix, activation_function)\n",
        "\n",
        "\n",
        "# ------------------------ Optimizer and Loss Function ------------------------\n",
        "\n",
        "optimizer, loss_fn = get_optimizer_and_loss(assignment_model)\n",
        "\n",
        "\n",
        "# ------------------------ Training Loop --------------------------------------\n",
        "\n",
        "# Lists to keep track of loss and accuracy for each epoch\n",
        "epoch_losses = []\n",
        "epoch_accuracies = []\n",
        "\n",
        "# Training Loop with 5 Epochs\n",
        "for epoch in range(epochs):\n",
        "    total_loss, total_acc = 0, 0\n",
        "    for texts, sentiments in train_loader:                                      # get next iteration from our dataloader\n",
        "        optimizer.zero_grad()                                                   # reset the gradients that have been calculated before\n",
        "        outputs = assignment_model(texts)                                       # get the model's output\n",
        "        loss = loss_fn(outputs, sentiments)                                     # calculte the new gradients according to the loss function\n",
        "        loss.backward()                                                         # the optimizer uses the gradients to update the model's parameters!\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss and accuracy (for monitoring)\n",
        "        total_loss += loss.item()\n",
        "        total_acc += (outputs.argmax(1) == sentiments).float().mean().item()\n",
        "\n",
        "    # Average loss and accuracy for the epoch\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_acc = total_acc / len(train_loader)\n",
        "\n",
        "    # Append to lists\n",
        "    epoch_losses.append(avg_loss)\n",
        "    epoch_accuracies.append(avg_acc)\n",
        "\n",
        "    print(f'Epoch {epoch+1}:\\tLoss: {avg_loss:.2f},\\tAccuracy: {avg_acc:.2f}')\n",
        "\n",
        "if epoch_accuracies[-1] < 0.8:\n",
        "  print(f\"\\nThe final accuracy on the validation data is less than 0.80. See if you can adjust the model/ the hyperparameters to get this value to 0.80!\")\n",
        "else:\n",
        "  print(f\"\\nThe final accuracy on the validation data is above 0.80!! Well done! The model seems to be able to recognise positive and negative sentiments on the data!\")\n",
        "\n",
        "# ------------------- Evaluate Model on Test Data -----------------------------\n",
        "\n",
        "assignment_model.eval()                                                         # Set the model to evaluation mode\n",
        "with torch.no_grad():                                                           # This means we are NOT calculating the gradients (since this is not a step for the training)\n",
        "    total_acc_test = 0\n",
        "    for texts, sentiments in test_loader:\n",
        "        outputs = assignment_model(texts)                                       # Get the model outputs (i.e. predictions)\n",
        "        total_acc_test += (outputs.argmax(1) == sentiments).float().mean().item() #\n",
        "\n",
        "    avg_acc_test = total_acc_test / len(test_loader)                            # Get an average accuracy from all the batches that were part of the\n",
        "    if avg_acc_test < 0.7:                                                      # Check if the average accuracy is lower than 0.7\n",
        "      print(f\"\\nAccuracy on the Test Data: {avg_acc_test:.2f}\\nSeems the model is struggling with new (never seen before data)! Try to get it above 0.70!\\n\")\n",
        "    else:\n",
        "      print(f\"\\nAccuracy on the Test Data: {avg_acc_test:.2f}\\nNice work! The model handles new data quite well!!\\n\")\n",
        "\n",
        "\n",
        "# ----------------- Create Charts for Loss and Accuracy -----------------------\n",
        "\n",
        "# Set up for the Loss and Accuracy Sub-plots\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Plotting the loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, epochs + 1), epoch_losses, marker='o', color='darkred', label='Training Loss')\n",
        "plt.title('Loss during Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0, max(epoch_losses) * 1.1)\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', color='grey', alpha=0.7)\n",
        "\n",
        "# Plotting the accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, epochs + 1), epoch_accuracies, marker='o', label='Training Accuracy', color='darkblue')\n",
        "plt.bar(epochs + 1, avg_acc_test, color='lightblue', label='Test Accuracy', width=0.5)\n",
        "plt.title('Accuracy on the Training and Test Data')\n",
        "plt.xlabel('Epoch')\n",
        "plt.xticks(list(range(1, epochs + 2)))\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "epoch_ticks = list(range(1, epochs + 1)) + [\"Test\"]\n",
        "plt.xticks(list(range(1, epochs + 2)), epoch_ticks)\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', color='grey', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bdKBVtbRdgCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Test the Trained Model on your own Input Text 🕵"
      ],
      "metadata": {
        "id": "LLKBypAidjTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- PREPARED FOR YOU, DO NOT EDIT --------------------------\n",
        "\n",
        "def predict_sentiment(model, sentence, vocab):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Tokenize and convert to indices\n",
        "    tokenized = tokenize_and_convert_to_indices(sentence)\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    indexed = torch.tensor([tokenized], dtype=torch.long)\n",
        "\n",
        "    # Prediction\n",
        "    with torch.no_grad():\n",
        "        predictions = model(indexed)\n",
        "\n",
        "    # Apply softmax to convert logits to probabilities\n",
        "    probabilities = torch.softmax(predictions, dim=1)\n",
        "\n",
        "    # Get the predicted class\n",
        "    prediction = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "    return prediction\n"
      ],
      "metadata": {
        "id": "JGyV5lYKdmHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- PREPARED FOR YOU, DO NOT EDIT --------------------------\n",
        "\n",
        "# Test the trained model on your own input!\n",
        "sentence = input(\"Now let's test the trained model on your own custom text!\\nEnter your own input:\\t\")\n",
        "\n",
        "predicted_sentiment = predict_sentiment(assignment_model, sentence, vocab)\n",
        "sentiment_mapping = {0: 'Negative', 1: 'Positive'}\n",
        "\n",
        "print(f\"\\nPredicted sentiment:\\t{sentiment_mapping[predicted_sentiment]}\")"
      ],
      "metadata": {
        "id": "QU_Dd-E_dpFy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}